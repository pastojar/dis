% !Rnw root = dis.Rnw

\chapter{On the value of CML QPEs for urban rainfall-runoff modelling: The pilot study} \label{chap3}

\rule{\textwidth}{0.4pt}
... \newline
....\newline
... \newline
\rule[0.2cm]{\textwidth}{0.4pt}

{\footnotesize The bulk of this chapter was originally published in: \newline
\-\hspace{0.5cm}
Pastorek, J., Fencl, M., Rieckermann, J., Sýkora, P., Stránský, D., Dohnal, M., Bareš, V. 2018. \textbf{Posouzení srážkových dat z mikrovlnných spojů v městském povodí pomocí analýzy nejistot hydrologického modelu} (in Czech). \emph{SOVAK: Časopis oboru vodovodů a kanalizací} 27, 16-‐22. \newline
\-\hspace{0.5cm}
Pastorek, J., Fencl, M., Stránský, D., Rieckermann, J., Bareš, V. 2017. \textbf{Reliability of microwave link rainfall data for urban runoff modelling}. \emph{Proceedings of the 14th IWA/IAHR International Conference on Urban Drainage}, Prague, Czech Republic, 10--15 September 2017, 1340-‐1343.
}
\newpage


\section{Objectives of the study}

...
....
...

 
\section{Material and Methods} \label{paperIMnM}

We investigated the defined problem on a small urban catchment in Prague-Letňany, Czech Republic. Our experimental design for rainfall monitoring (Fig. X) enabled us to investigate the following rainfall data sets: 
\begin{enumerate}
        \item A single temporary rain gauge in the catchment (RG2),
        \item QPEs from four CMLs adjusted by data from RG2, 
        \item Three rain gauges from the long-term municipal network,
        \item QPEs from four CMLs adjusted by the three municipal gauges,
        \item Unadjusted QPEs from all CMLs available in the area.
\end{enumerate}
For details on the CML data adjustment method, see \cite{fenclGaugeadjustedRainfallEstimates2017}. 

We used 10 events from the summer season of 2014 to analyze the model predictions. To better interpret the results, we classified the rainfall events as either light or heavy, based on the maximal 10-minute precipitation rate. 

Prediction uncertainty was estimated by a method that statistically describes model deficiencies (Del Giudice et al., 2013). Specifically, model bias, induced chiefly by input and structural errors, is in this method represented as a stochastic autocorrelated process. By varying the model input (rainfall data) while keeping the model structure unchanged, we were able to trace the associated uncertainty changes back to the input (rainfall) data. See the section \ref{paperIUnc} for more details.

To assess the model performance, we used the following metrics: i) the relative error of the total runoff volume ($dV$ [-]), ii) the rel. error of the maximal discharges (integrated over 8-min period) ($dVpeak$ [-]), iii) the timing of the discharge maximum ($dt_{Qmax}$ [h]), and iv) the prediction reliability ($reliab$~[-]), i.e. the fraction of flow observations falling into the predicted interval.


\section{Prediction uncertainty quantification} \label{paperIUnc}

Prediction uncertainty of the rainfall-runoff modelling is estimated by a method first used in a similar context by \cite{giudice2013improving}. The basic principle of the chosen method is the extension of the deterministic rainfall-runoff model by a stochastic error model. However, a commonly used error model considering only independent and identically distributed (i.i.d.) errors is adjusted to explicitly account for the bias of the rainfall-runoff model. By combining the deterministic hydrological and the stochastic error models, we can quantify the probability that the observed runoffs can be explained by the given predicted runoffs and error model. This can be formally expressed by a likelihood function describing the joint probability density of the observed system outcomes (i.e. the extended model). To achieve accurate rainfall-runoff predictions and reliable quantification of their uncertainty, the extended model should be calibrated. In theory, this could be done by optimizing the likelihood function as the objective function. However, by implementing the Bayesian approach, i.e. combining the likelihood with prior knowledge (belief) about the extended model, we can ...

The extended model can be thus formulated using the equation
\begin{equation}
\tilde{Y} (x,\Theta, \psi) = \tilde{y} (x, \Theta) + B (\psi) + E (\psi)
\end{equation}
where $\tilde{Y}$ is the transformed observed system output, $\tilde{y}$ represents the transformed deterministic model predictions, $B$ stands for the systematic model errors (bias) and $E$ for the i.i.d. errors. Precipitation as the external driving force is represented by $x$, whereas $\Theta$ and $\psi$ respectively represent the deterministic and error model parameters.



Representing the measurement noise of the system response, $E$ is sampled from a multivariate normal distribution with mean 0 and a diagonal covariance matrix
\begin{equation}
\Sigma_E= \sigma_E^2 \mathds{1}
\end{equation}


The bias $B$ is formulated as an autoregressive stationary random process with a long-term equilibrium value of zero and a constant variance. \enquote{It is a mean-reverting OU process \citep{uhlenbeck1930theory}, the discretisation of which would be a first-order autoregressive process with Gaussian independent and identically distributed noise} \citep{giudice2013improving}. It can be expressed using the following differential equation:
\begin{equation} 
dB (t)= - \frac{B (t)}{\tau}dt + \sqrt{\frac{2}{\tau}} \sigma_{B_{ct}}  dW(t),
\end{equation}
where $\tau$ represents the correlation time, $\sigma_{B_{ct}}$ the asymptotic standard deviation of the random fluctuations around the equilibrium and $dW(t)$ a Wiener process (standard Brownian motion). Although there has been some research on using more sophisticated, e.g. model input- or output-dependent, bias formulation \citep{honti2013integrated}, we have decided to follow the recommendations of \cite{giudice2013improving} and to employ the simpler constant bias formulation.

Output transformation


The likelihood function combines the deterministic hydrological model with a stochastic error term and describes the joint probability density $ f (y_{o}|\Theta, \psi, x) $ of observed system outcomes ($y_{0}$) for given (simulator and error model) parameters  and external driving forces (precipitation, $x$). The equation can be written as

$ f (y_{o}|\Theta, \psi, x) = $
\begin{equation}
 \frac{(2\Pi)^{-\frac{n}{2}}}{\sqrt{det(\Sigma(\Theta, \psi, x))}}  . exp \Big( -\frac{1}{2}  [ \tilde{y}_{o} - \tilde{y}_{M}(\Theta, x)^{T}  \Sigma(\Theta, \psi, x)^{-1} ]  [ \tilde{y}_{o} - \tilde{y}_{M}(\Theta, x)]  \prod^{n}_{i=1} \frac{dg}{dy} (y_{o,i},\psi) \Big),
\end{equation}

where $\Sigma(\Theta, \psi, x)$ stands for a covariance matrix of the residuals transformed by a function $g()$, i.e. $\tilde{y}=g(y)$. Observed values are represented by $y_{o}$, deterministic model predictions by $y_{M}$ and $n$ is the number of observations (the dimension of $y_{o}$ and $y_{M}$).


Calibration of our deterministic model (parameters $\Theta$) expanded by the error model (parameters $\psi$) using the statistical bias description method and subsequent analysis of prediction uncertainties require to follow these steps \citep{giudice2013improving}:
\begin{itemize}
	\item  definition of the prior distributions of the parameters
	\item  obtaining the posterior distributions with Bayesian inference
	\item  probabilistic predictions for  data used for calibration
	\item  probabilistic predictions for unseen data (extrapolation)
	\item  assessment of the predictions quality
	\item verification of the statistical assumptions
\end{itemize}



\subsection{Prior definition}
The first step is to define marginal distributions of the prior joint probability distribution of the parameters of  both the deterministic hydrological model ($\Theta$) and the error model ($\psi$, i.e. $\tau$,  $\sigma_{B_{ct}}$ and  $\sigma_E$). 

In accordance with \cite{giudice2013improving}, we do not use  completely uninformative priors. For example, we recognize that $\sigma_{B_{ct}}$ is unlikely to be higher than the variability of observed discharge, $\tau$ should represent the characteristic correlation length of the residuals and  $\sigma_E$ mirrors the measurement noise of the system output. Furthermore, according to \cite{giudice2013improving}, it is \enquote{important that the prior of the bias reflects the desire to avoid model inadequacy as much as possible}. 

When the parameters of the deterministic hydrological model ($\Theta$) are considered, they represent multiplicative (scaling) values of  parameters of the empirical model. More information on these parameters can be found in the section \ref{implem}.  The priors for the used parameters (both $\Theta$ and  $\psi$) have been chosen based on consultations with experts who had already used the method in the past. 

For all the parameters (both $\Theta$ and $\psi$), the marginals of the prior joint distribution were defined as truncated normal distributions with 4 defining parameters - mean, standard deviation, minimum and maximum. It should be noted that  $\sigma_{B_{ct}}$ and  $\sigma_E$ are defined in a transformed space (via a transformation $g()$). The priors used for purpose of this thesis are summarized in the following table:

\begin{table}[H]
\centering
\begin{tabular}{ c | c | c | c | c }
	$par$		&	$\mu$ & 	$\sigma$	& 	$min$ 			& 	 $max$  \\ \hline \hline

	$imp$		&	1	&	1		&	0.8			&	1.2		\\ \hline
	$wid$		&	1	&	1		&	0.3			&	1.7		\\ \hline
	$Nim$		&	1	&	1		&	0.3			&	1.7		\\ \hline
	$Sim$		&	1	&	1		&	0.3			&	1.7		\\ \hline
	$pc0$		&	1	&	1		&	0.3			&	1.7		\\ \hline
	$Nco$		&	1	&	1		&	0.3			&	1.7		\\ \hline
	
	$\tau$		&	0.5		&	0.25		&	0.01			&	3	\\ \hline
	$\sigma_E$		&	$g($0.5$)$	&	0.25		&	0.01			&	1.5	\\ \hline
	$\sigma_{B_{ct}}$	&	$g($50$)$	&	25		&	0			&	10 000 000
	
	
\end{tabular}
\caption{Summary of the prior marginal distributions.}
\label{kalibracia_tab}
\end{table}



\subsubsection{Calibration using Bayesian inference} \label{calib}
Another step is to obtain the posterior joint distribution of the parameters $ f (\Theta, \psi | y_o, x) $. Using the notation of \cite{giudice2013improving}, we can write the Bayes' theorem as
\begin{equation}
 f (\Theta, \psi | y_o, x) = \frac{ f(\Theta, \psi) \, f(y_o | \Theta, \psi, x) } { \iint f(\Theta' , \psi') \, f(y_o | \Theta', \psi', x) d\Theta' d\psi'} \; ,
\end{equation}
where $f(\Theta, \psi)$ is the prior distribution  and $ f(y_o | \Theta, \psi, x)$ the likelihood function.

Using this relation, the joint probability density, a product of the prior and the likelihood function, gets conditioned on the data. It is an iterative process, meaning that the posterior distribution of the step $i-1$ serves as prior for the step $i$.

To solve this problem analytically would include dealing with multidimensional integrals. This is avoided by employing a numerical Markov Chain Monte Carlo (MCMC) method to approximate properties of the posterior distribution. More details on the exact implementation for our case are specified in the section \ref{implem}.



\subsubsection{Probabilistic predictions}
The third step is to compute predictive distributions for the data points (observations) that have been used for the calibration process. It should be noted that the word \enquote{predictions} in this context represents  generation of model outputs in general, consistently with e.g. \cite{reichert2012linking} or \cite{giudice2013improving}, in contrast to simulation only for time points or locations where measurements are not available. In other words, using the terminology of \cite{breinholt2012formal}, our model is tailored as an off-line simulation model suitable rather for long-term investigations than for forecasting in  real time.

For details on how to calculate probabilistic predictions for multivariate normal distributions related to the random variables of our type,  \cite{giudice2013improving} recommend to consult \cite{kendall1994vol} or \cite{kollo2006advanced}.

Subsequently, one should calculate predictive distributions for unseen temporal points (observations not used for the calibration process), also called \enquote{extrapolation layout} by \cite{giudice2013improving}. It is possible to proceed analogically as outlined above. However, \cite{giudice2013improving} suggest to take advantage of using bias formulated as an  OU process and to \enquote{draw a realization for the entire period by iteratively drawing the realization for the next time step at time   $t_ j$ from that of a previous time step at time $t_ {j-1}$ from a normal distribution}.

In both cases, nevertheless, it is necessary to draw a large sample from the  posterior parameter joint distribution and to propagate it through the deterministic model. Separating different uncertainty components is enabled by expanding the transformed simulator output $\tilde{y}_M$ with the model bias $B_M$ and independent error $E$ terms (see the equation (3.2)). Subsequently, the results are transformed back to the original space using the inverse transformation $g^{-1}()$.

To be able to visualize and distinguish uncertainty of the deterministic simulator predictions $y_M$, the best knowledge about the system response $g^{-1}(\tilde{y}_M+ B_M)$ and the modelled observed system response (including flow measurement errors) $Y_o= g^{-1}(\tilde{y}_M+ B_M + E)$, it is profitable to compute sample quantiles (e.g. $0.05$, $0.5$ and $0.095$) of the respective calculated predictions in every time step.



\subsection{Performance assessment}
To conclude uncertainty analysis, quality of the predictions (propagations of the sample from the posterior joint parameter distribution) should be evaluated and underlying statistical assumptions verified \citep{giudice2013improving}. 

When dealing with interval predictions at a certain confidence level $1-\alpha$ (determined by the predictive quantiles at level $ \frac{\alpha}{2} $ and $ 1-\frac{\alpha}{2} $) as in our case, it is common to evaluate the model predictive performance by two metrics: width of the determined confidence interval, referred to also as interval sharpness \mbox{\citep[e.g.][]{breinholt2012formal}} and reliability of the predictions represented by the share the observed data included within the confidence bounds.

\cite{giudice2013improving} assessed the model predictive capability also in this manner, evaluating the uncertainty bands  by calculating the predictions reliability and the \enquote{average bandwidth} --  the interval widths averaged over the entire prediction period.

We have decided to employ, apart from the two mentioned above,  as well a metric which combines them -- the interval score $S_\alpha$ as formulated by \cite{gneiting2007strictly}. For a single interval prediction, the interval score 
\begin{equation}
S_\alpha(l,u,x) =  ( u - l ) + \frac{2}{\alpha} (l - x)  \mathds{1} \left\{x < l \right\} + \frac{2}{\alpha} (x - u)  \mathds{1} \left\{x > u \right\} ,
\end{equation}
where $l$ and $u$ stand for lower and upper interval bounds (quantiles at levels $ \frac{\alpha}{2} $ and $ 1-\frac{\alpha}{2} $ ). The evaluated variable $x$ stands in our case for the modelled system response including measurement errors $Y_o$. 

As described by \cite{gneiting2007strictly}, when using the interval score $S_\alpha$, \enquote{the forecaster is rewarded for narrow prediction intervals, and he or she incurs a penalty, the size of which depends on $\alpha$, if the observation misses the interval}, which should provide it with an intuitive appeal.

However, we obtain predictions in a from of a vector of discharges in various time steps of a rain event. Therefore, we \enquote{extend} the idea of  $S_\alpha$ and evaluate the mean of interval scores ($M\!I\!S$) for every given   rain event. Analogically to this, we calculate as well the \enquote{average band width} ($A\!B\!W$) as the mean width of prediction intervals in the given period (similarly to \cite{giudice2013improving}).

We do not the evaluate the prediction performance  by examining separately the uncertainty intervals for the deterministic model output. This is again in accordance with \cite{giudice2013improving}, who see this approach as \enquote{not conclusive because the field observations are not realisations of the deterministic model but of the model plus the errors}.

Apart from assessing the model  predictive performance using the modelled discharge values at every time step of the prediction period, we investigate as well metrics more common in everyday engineering practice, such as the total discharged volume over the observed period $V$ \citep[e.g.][]{fencl2013assessing}, the discharged volume during peak flow period $V_{peak}$  and Nash–Sutcliffe efficiency index $N\!S\!E$ \citep{nash1970river}. To be more precise, we compute the relative error of the total discharged volume and of the volume during peak flow ($dV$ and $dV_{peak}$) for every prediction. 

When calculating $V_{peak}$, the time step with maximal discharge (can differ for observation and prediction) from the given period is identified first and the volume discharged during $4$ min around the time step (\mbox{$2$ min} before and $2$ min after) is computed afterwards. The difference between the observed time with maximal discharge and the modelled one (the \enquote{time shift}) is another metric we record for every prediction.

We calculate as well the mean and the standard deviation of the conventional metrics described above ($dV$, $dV_{peak}$, $N\!S\!E$, time shift of the peak). We see it as another way how to evaluate the model  predictive uncertainty. This is in accordance with \cite{fencl2013assessing}, who use as well the relative error of the total discharged volume and who interpret the related mean ($E(dV)$) as the metric's  bias and the standard deviation  ($sd(dV)$) as its uncertainty. 

However, we acknowledge that  uncertainty estimation based on  standard deviations of  predictions performance metrics (e.g. $dV$)  is reliable only as far as the predictions themselves are reliable. If the reliability is too low (below the defined confidence level $1- \alpha$), the $sd$ of a given metric does not reflect the metric's uncertainty reliably. In this case, the interval score $S_\alpha$ (expanded to  $M\!I\!S$ in our case) proves its worth, since it combines the uncertainty estimation based purely on predictions with the reliability obtained after comparing the predictions with observations.

In many similar cases, it is usual to confirm the statistical assumptions of the error model by residual analysis \citep{reichert2012linking}. However, Bayesian approach implemented in this method allows us to test only the observation error $E$, which is the only purely frequentist term. 

According to \cite{giudice2013improving}, this test can be performed on the predictive distribution of the observation error, predicted for the calibration data. It should be verified whether the observation errors are normally distributed with constant variance and without autocorrelation. However, since these errors are likely to constitute only a small portion of the residuals of the deterministic simulator, the informative value of this analysis might be limited.


